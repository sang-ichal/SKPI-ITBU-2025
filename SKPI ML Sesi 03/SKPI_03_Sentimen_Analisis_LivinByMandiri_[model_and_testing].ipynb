{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP54TzKiYjtZAAzojXQzeYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sang-ichal/SKPI-ITBU-2025/blob/main/SKPI%20ML%20Sesi%2003/SKPI_03_Sentimen_Analisis_LivinByMandiri_%5Bmodel_and_testing%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANALISIS SENTIMEN PENGGUNA TERHADAP APLIKASI LIVIN BY MANDIRI DI PLAY STORE - MODEL DAN TESTING**\n",
        "**by FAIZAL RIZA [ INSTITUT TEKNOLOGI BUDI UTOMO ]**"
      ],
      "metadata": {
        "id": "GhWws0iPfO5a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRocOwNQqY1M"
      },
      "source": [
        "# **1. KRITERIA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-ESF-Fbqc2H"
      },
      "source": [
        "**1.1. WAJIB**\n",
        "\n",
        "| No  |  Kriteria | Dipenuhi  | Catatan  |\n",
        "|---|---|---|---|\n",
        "| 1  | Data merupakan hasil scraping dengan jumlah minimal 3000 sampel  | Ya  | Dataset hasil scraping mandiri (terlampir scraping-nya : **Scraping_Data.ipynb** dan **hasil_scraping_livin.csv**) |\n",
        "| 2  | Melakukan tahapan ekstraksi fitur dan pelabelan data  | Ya  | Pada Tahap **3. Pelabelan dan Ekstraksi Fitur**  |\n",
        "| 3  | Menggunakan algoritma pelatihan machine learning  | Ya  | Pada Tahap **5. Pemodelan**  |,\n",
        "| 4  | Akurasi testing set yang didapatkan untuk setiap skema pelatihan minimal harus mencapai 85%  | Ya  | Dapat dilihat pada **5.6 Perbandingan Akurasi Model**  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWSE_xWkusHQ"
      },
      "source": [
        "**1.2. TAMBAHAN**\n",
        "\n",
        "| No  |  Kriteria | Dipenuhi  | Catatan  |\n",
        "|---|---|---|---|\n",
        "| 1  | Menggunakan algoritma deep learning selain Naive Bayes (NB), Random Forest (RF), Logistic Regression (LR), dan Decision Tree (DT)  | Ya  | Menggunakan **5.2 Support Vector Machine**, **5.3 LightGBM**, **5.4 XGBoost** dan **5.5 CatBoost** |\n",
        "| 2  | Dataset memiliki minimal tiga kelas  | Ya | Positive, Negative dan Neutral  |\n",
        "| 3  | Memiliki jumlah data minimal 10.000 sampel data  | Ya  | Data yang digunakan 150.000 sampel Cek **2.3 Dataset**  |\n",
        "| 4  | Melakukan inference atau testing dalam file .ipynb yang menghasilkan output berupa kelas kategorikal (contoh : negatif, netral, dan positif)  | Ya  | Dapat dilihat pada **Testing.ipynb**  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Lzenlkw_Tn"
      },
      "source": [
        "# **2. PREPARATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18hHGxVFx_pv"
      },
      "source": [
        "**2.1. Instal dan Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQGqHSNAqLZr"
      },
      "outputs": [],
      "source": [
        "!pip install sastrawi  # Menginstal pustaka Sastrawi untuk stemming bahasa Indonesia\n",
        "!pip install tensorflow  # Menginstal TensorFlow untuk pembelajaran mesin dan deep learning\n",
        "!pip install scikit-learn  # Menginstal Scikit-learn untuk alat analisis data dan pembelajaran mesin\n",
        "!pip install pandas nltk wordcloud requests matplotlib seaborn # Menginstall paket lainnya\n",
        "!pip install lightgbm xgboost catboost # Menginstall paket algoritma\n",
        "\n",
        "#import library yang diperlukan\n",
        "import pandas as pd  # Mengimpor pustaka pandas untuk manipulasi data\n",
        "import re  # Mengimpor pustaka re untuk operasi regex (regular expression)\n",
        "from nltk.tokenize import word_tokenize  # Mengimpor fungsi tokenisasi kata dari NLTK\n",
        "from nltk.corpus import stopwords  # Mengimpor daftar stopwords dari NLTK\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Mengimpor pabrik Stemmer dari Sastrawi\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Mengimpor pabrik StopWordRemover dari Sastrawi\n",
        "from wordcloud import WordCloud  # Mengimpor WordCloud untuk visualisasi kata\n",
        "import nltk  # Mengimpor pustaka NLTK\n",
        "nltk.download('punkt')  # Mengunduh tokenizer NLTK\n",
        "nltk.download('stopwords')  # Mengunduh daftar stopwords NLTK\n",
        "import requests  # Mengimpor pustaka requests untuk melakukan permintaan HTTP\n",
        "import json  # Mengimpor pustaka json untuk bekerja dengan data JSON\n",
        "import string  # Mengimpor pustaka string untuk operasi string\n",
        "import matplotlib.pyplot as plt  # Mengimpor pyplot dari matplotlib untuk visualisasi data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Mengimpor TfidfVectorizer dari scikit-learn untuk ekstraksi fitur teks\n",
        "from sklearn.model_selection import train_test_split  # Mengimpor fungsi pembagian data dari scikit-learn\n",
        "from sklearn.metrics import accuracy_score, precision_score  # Mengimpor metrik akurasi dan presisi dari scikit-learn\n",
        "import time  # Mengimpor pustaka time untuk pengukuran waktu\n",
        "import numpy as np  # Mengimpor pustaka numpy untuk operasi numerik\n",
        "import seaborn as sns # Import seaborn library as sns\n",
        "from sklearn.svm import SVC # Support Vector Machine (SVM)\n",
        "import lightgbm as lgb # LightGBM\n",
        "from xgboost import XGBClassifier # XGBoost\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from catboost import CatBoostClassifier # CatBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPguU_Ity09E"
      },
      "source": [
        "**2.2. Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zraq2jLZy7m0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/sang-ichal/datasetlivin/main/hasil_scraping_livin.csv', encoding='utf-8', quotechar='\"', escapechar='\\\\')\n",
        "# Membaca file CSV dari URL ke dalam DataFrame pandas\n",
        "# encoding='utf-8': Mengatur encoding karakter ke UTF-8\n",
        "# quotechar='\"': Mengatur karakter kutipan untuk nilai-nilai teks\n",
        "# escapechar='\\\\': Mengatur karakter pelolosan untuk menghindari kesalahan parsing\n",
        "\n",
        "df.head() # Menampilkan 5 baris pertama dari DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNLJkHuD56rg"
      },
      "source": [
        "**2.3. Data Cleansing / Pembersihan Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cswoWiY6BL3"
      },
      "outputs": [],
      "source": [
        "# Menghapus baris yang mengandung nilai null dari DataFrame dan Hasilnya disimpan ke dalam DataFrame baru bernama clean_df\n",
        "clean_df = df.dropna()\n",
        "\n",
        "# Menghapus baris duplikat dari DataFrame dan Hasilnya disimpan ke dalam DataFrame baru bernama clean_df\n",
        "clean_df = clean_df.drop_duplicates()\n",
        "\n",
        "# Menampilkan informasi ringkas tentang DataFrame clean_df Termasuk jumlah baris, kolom, tipe data masing-masing kolom, dan jumlah nilai non-null\n",
        "clean_df.info()\n",
        "\n",
        "# Menampilkan 5 baris pertama dari DataFrame\n",
        "clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk5owC9v6mDT"
      },
      "source": [
        "**2.4. Pra-Pemrosesan Teks / Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7bGjt_J6un2"
      },
      "outputs": [],
      "source": [
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Menghapus mention (@username)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)  # Menghapus hashtag (#hashtag)\n",
        "    text = re.sub(r\"http\\S+\", '', text)  # Menghapus URL\n",
        "    text = re.sub(r'[0-9]+', '', text)  # Menghapus angka\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Menghapus karakter non-alfanumerik kecuali spasi\n",
        "    text = text.replace('\\n', ' ')  # Mengganti newline dengan spasi\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Menghapus tanda baca\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"livin\", \"mandiri\", \"mandiri mobile\"]])  # Menghapus kata-kata yang diinginkan\n",
        "    text = text.strip(' ')  # Menghapus spasi di awal dan akhir teks\n",
        "    return text  # Mengembalikan teks yang telah dibersihkan\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()  # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
        "    return text  # Mengembalikan teks yang telah di-casefold\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)  # Memecah teks menjadi token (kata-kata)\n",
        "    return text  # Mengembalikan daftar token\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))  # Mengambil daftar stopwords bahasa Indonesia\n",
        "    listStopwords1 = set(stopwords.words('english'))  # Mengambil daftar stopwords bahasa Inggris\n",
        "    listStopwords.update(listStopwords1)  # Menggabungkan dua daftar stopwords\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:  # Memeriksa apakah kata bukan stopword\n",
        "            filtered.append(txt)  # Menambahkan kata yang bukan stopword ke daftar filtered\n",
        "    text = filtered  # Menggantikan teks dengan daftar kata yang telah difilter\n",
        "    return text  # Mengembalikan daftar kata yang telah difilter\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()  # Membuat pabrik stemmer\n",
        "    stemmer = factory.create_stemmer()  # Membuat stemmer\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]  # Melakukan stemming pada setiap kata\n",
        "    stemmed_text = ' '.join(stemmed_words)  # Menggabungkan kata-kata yang telah distem menjadi satu kalimat\n",
        "    return stemmed_text  # Mengembalikan teks yang telah distem\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)  # Menggabungkan daftar kata menjadi satu kalimat\n",
        "    return sentence  # Mengembalikan kalimat yang dibentuk dari daftar kata\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    fixed_words = []  # Inisialisasi daftar untuk kata-kata yang sudah diperbaiki\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:  # Memeriksa apakah kata tersebut adalah kata slang\n",
        "            fixed_words.append(slangwords[word.lower()])  # Jika ya, mengganti kata slang dengan kata standar\n",
        "        else:\n",
        "            fixed_words.append(word)  # Jika bukan kata slang atau kata yang ingin dihapus, tetap mempertahankan kata tersebut\n",
        "    fixed_text = ' '.join(fixed_words)  # Menggabungkan kata-kata yang sudah diperbaiki menjadi satu teks\n",
        "    return fixed_text  # Mengembalikan teks yang sudah diperbaiki"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mengambil Data Kamus Kata Baru Tidak Baku (Slang)**"
      ],
      "metadata": {
        "id": "5OE73Y_Tg9wA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Iz_U1W6yge"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/sang-ichal/datasetlivin/main/slangwords.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)  # Mengirim permintaan HTTP GET ke URL untuk mendapatkan konten\n",
        "\n",
        "if response.status_code == 200:  # Jika status code respons adalah 200 (OK)\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)  # Mengurai JSON yang diperoleh dari respons dan menyimpannya dalam variabel slangwords\n",
        "    except json.JSONDecodeError as e: # Menangani kesalahan jika terjadi masalah saat mengurai JSON\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code) # Menampilkan pesan kesalahan jika tidak berhasil mengambil data dari URL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prose Text Cleansing**"
      ],
      "metadata": {
        "id": "q__HlAechOcE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0sRdvT87NVg"
      },
      "outputs": [],
      "source": [
        "clean_df['text_clean'] = clean_df['content'].apply(cleaningText) # Membersihkan teks pada kolom 'Content' menggunakan fungsi cleaningText dan menyimpan hasilnya dalam kolom baru 'text_clean'\n",
        "clean_df['text_casefoldingText'] = clean_df['text_clean'].apply(casefoldingText) # Melakukan case folding pada teks yang sudah dibersihkan dan menyimpan hasilnya dalam kolom 'text_casefoldingText'\n",
        "clean_df['text_slangwords'] = clean_df['text_casefoldingText'].apply(fix_slangwords) # Memperbaiki kata-kata slang dalam teks yang sudah di-casefold dan menyimpan hasilnya dalam kolom 'text_slangwords'\n",
        "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText) # Melakukan tokenisasi pada teks yang sudah diperbaiki slangwords dan menyimpan hasilnya dalam kolom 'text_tokenizingText'\n",
        "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText) # Melakukan filtering stopwords pada teks yang sudah di-tokenisasi dan menyimpan hasilnya dalam kolom 'text_stopword'\n",
        "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence) # Menggabungkan kata-kata yang sudah difilter stopwords menjadi satu kalimat dan menyimpan hasilnya dalam kolom 'text_akhir'\n",
        "\n",
        "clean_df.head() # Menampilkan 5 baris pertama dari DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Memanggil Skor Leksikon Positif dan Negatif**"
      ],
      "metadata": {
        "id": "w1VgovdGhbTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nj4NyYG76jS"
      },
      "outputs": [],
      "source": [
        "def load_lexicon(url):\n",
        "    lexicon = dict()  # Inisialisasi kamus untuk menyimpan kata dan skornya\n",
        "    response = requests.get(url)  # Mengirim permintaan HTTP GET ke URL untuk mendapatkan konten\n",
        "\n",
        "    if response.status_code == 200:  # Jika status code respons adalah 200 (OK)\n",
        "        lines = response.text.strip().split('\\n')  # Memisahkan konten menjadi baris-baris\n",
        "        for line in lines:\n",
        "            parts = line.split(',')  # Memisahkan baris menjadi bagian-bagian berdasarkan koma\n",
        "            word = ','.join(parts[:-1])  # Menggabungkan bagian-bagian kecuali bagian terakhir (kata)\n",
        "            score = parts[-1]  # Mengambil bagian terakhir sebagai skor\n",
        "            lexicon[word.strip()] = int(score.strip())  # Menyimpan kata dan skornya dalam kamus, mengonversi skor ke integer\n",
        "    else:\n",
        "        print(f\"Failed to fetch lexicon data from {url}\")  # Menampilkan pesan kesalahan jika tidak berhasil mengambil data dari URL\n",
        "\n",
        "    return lexicon  # Mengembalikan kamus yang berisi kata-kata dan skornya\n",
        "\n",
        "positive_lexicon_url = 'https://raw.githubusercontent.com/sang-ichal/datasetlivin/main/LeksikonPositif.txt'\n",
        "negative_lexicon_url = 'https://raw.githubusercontent.com/sang-ichal/datasetlivin/main/LeksikonNegatif.txt'\n",
        "\n",
        "# Memuat leksikon positif dan negatif dari URL yang diberikan\n",
        "lexicon_positive = load_lexicon(positive_lexicon_url)\n",
        "lexicon_negative = load_lexicon(negative_lexicon_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penskoran Berdasarkan Akumulasi Leksikon Positif dan Negatif**"
      ],
      "metadata": {
        "id": "UsDiRxQ5hgN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYaawuPY8fve"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis_lexicon_indonesia(text):\n",
        "    score = 0  # Inisialisasi skor sentimen\n",
        "\n",
        "    for word in text:\n",
        "        if (word in lexicon_positive):  # Jika kata ada dalam leksikon positif\n",
        "            score = score + lexicon_positive[word]  # Tambahkan skor positif kata tersebut ke skor total\n",
        "\n",
        "    for word in text:\n",
        "        if (word in lexicon_negative):  # Jika kata ada dalam leksikon negatif\n",
        "            score = score + lexicon_negative[word]  # Tambahkan skor negatif kata tersebut ke skor total\n",
        "\n",
        "    polarity = ''  # Inisialisasi polaritas sentimen\n",
        "\n",
        "    if (score > 0):\n",
        "        polarity = 'positive'  # Jika skor positif, tentukan polaritas positif\n",
        "    elif (score < 0):\n",
        "        polarity = 'negative'  # Jika skor negatif, tentukan polaritas negatif\n",
        "    else:\n",
        "        polarity = 'neutral'  # Jika skor nol, tentukan polaritas netral\n",
        "\n",
        "    return score, polarity  # Mengembalikan skor sentimen dan polaritasnya"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Menentukan Polaritas Kalimat Berdasarkan Leksikon Menjadi Class Positif, Negatif atau Netral**"
      ],
      "metadata": {
        "id": "9eD-4XGJhpsb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Xhql5CV8hMz"
      },
      "outputs": [],
      "source": [
        "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
        "# Menggunakan fungsi sentiment_analysis_lexicon_indonesia untuk menghitung skor sentimen dan polaritas untuk setiap teks dalam kolom 'text_stopword'\n",
        "# Hasilnya disimpan dalam variabel results sebagai objek Series yang berisi tuple (skor, polaritas)\n",
        "\n",
        "results = list(zip(*results)) # Membongkar tuple hasil menjadi dua list terpisah: satu untuk skor sentimen dan satu lagi untuk polaritas\n",
        "\n",
        "clean_df['polarity_score'] = results[0] # Menyimpan skor sentimen dari hasil analisis ke dalam kolom baru 'polarity_score' pada DataFrame clean_df\n",
        "\n",
        "clean_df['polarity'] = results[1] # Menyimpan polaritas sentimen dari hasil analisis ke dalam kolom baru 'polarity' pada DataFrame clean_df\n",
        "\n",
        "print(clean_df['polarity'].value_counts()) # Mencetak jumlah kemunculan setiap kategori polaritas sentimen ('positive', 'negative', 'neutral') dari kolom 'polarity' dalam DataFrame clean_df\n",
        "\n",
        "# clean_df.to_csv('labeled_text.csv', encoding='utf8', index=False)\n",
        "#unduh dataset yang sudah diberi label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simpan Hasil Polaritas Leksikon**"
      ],
      "metadata": {
        "id": "hLeWS-UQh2PA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYNnI9WmiPBz"
      },
      "outputs": [],
      "source": [
        "X = clean_df['text_akhir'] # Menyimpan nilai pada dataframe text_akhir ke variabel X\n",
        "y = clean_df['polarity'] # Menyimpan nilai pada dataframe polarity ke variabel y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khs1pT8cyFIP"
      },
      "source": [
        "# **3.2 Ekstraksi Fitur**\n",
        "Dengan ini, model akan mempertimbangkan unigram, bigram, dan trigram sebagai fitur.\n",
        "\n",
        "Contohnya, dengan kalimat \"Aplikasi Livin sangat membantu\", fitur-fitur yang dihasilkan akan meliputi:\n",
        "\n",
        "*   Unigram: \"aplikasi\", \"livin\", \"sangat\", \"membantu\"\n",
        "*   Bigram: \"aplikasi Livin\", \"Livin sangat\", \"sangat membantu\"\n",
        "*   Trigram: \"aplikasi Livin sangat\", \"Livin sangat membantu\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyAWqnAwyTuK"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer(max_features=200, min_df=17, max_df=0.8, ngram_range=(1, 1))\n",
        "X_tfidf = tfidf.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3AkdMmdy0yB"
      },
      "source": [
        "Penjelasan kode :\n",
        "\n",
        "\n",
        "*   **X_tfidf.toarray():** Mengubah matriks sparse X_tfidf (hasil dari TfidfVectorizer) menjadi array NumPy yang padat. Ini dilakukan agar data dapat lebih mudah diolah dan divisualisasikan.\n",
        "* **tfidf.get_feature_names_out():** Mengembalikan daftar nama fitur (kata atau n-gram) yang digunakan oleh TfidfVectorizer.\n",
        "*   **pd.DataFrame(...):** Membuat DataFrame Pandas dari array NumPy dan daftar nama fitur. Setiap baris dalam DataFrame mewakili dokumen (misalnya, ulasan), dan setiap kolom mewakili fitur (kata atau n-gram). Nilai dalam DataFrame adalah skor TF-IDF, yang menunjukkan seberapa penting fitur tersebut untuk dokumen tertentu.\n",
        "*   **features_df:** Menyimpan DataFrame yang dihasilkan dalam variabel features_df.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkG9cAWJzJ27"
      },
      "outputs": [],
      "source": [
        "features_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7oiUdnZzlUr"
      },
      "source": [
        "## **4. Data Visualization**\n",
        "\n",
        "**4.1 Persentase Polaritas Tiap Sentimen**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ1C196_ztOF"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 8)) # Mengatur ukuran figure\n",
        "sizes = [count for count in clean_df['polarity'].value_counts()] # Mengambil jumlah kemunculan setiap kategori polaritas sentimen dari kolom 'polarity' dalam DataFrame clean_df\n",
        "#labels = list(clean_df['polarity'].value_counts().index)  # Mengambil nama kategori polaritas sentimen\n",
        "labels = ['Negatif', 'Positif', 'Netral'] # Mengambil nama kategori polaritas sentimen dengan label yang sudah diubah\n",
        "explode = (0, 0, 0)  # Menentukan seberapa jauh potongan (slice) akan dikeluarkan (tidak ada potongan yang dikeluarkan)\n",
        "ax.pie(x=sizes, labels=labels, autopct='%1.1f%%', explode=explode, textprops={'fontsize': 14}) # Membuat pie chart dengan menggunakan data sizes dan labels\n",
        "ax.set_title('Polaritas Sentimen', fontsize=20, pad=22) # Menetapkan judul pada plot\n",
        "plt.show() # Menampilkan plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW2BkCc0z1l_"
      },
      "source": [
        "**4.2 Word Cloud**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXkg2mlf0A0g"
      },
      "outputs": [],
      "source": [
        "positive_review = clean_df[clean_df['polarity'] == 'positive'] # Memfilter hanya data dengan polaritas positif\n",
        "positive_review = positive_review[['text_akhir', 'polarity_score', 'polarity', 'text_stopword']] # Memilih kolom yang ingin ditampilkan\n",
        "positive_review = positive_review.sort_values(by='polarity_score', ascending=False) # Mengurutkan berdasarkan skor polaritas secara menurun\n",
        "positive_review = positive_review.reset_index(drop=True) # Mengatur ulang indeks\n",
        "positive_review.index += 1 # Menambahkan nomor urut dari 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_ay2COQ0Flw"
      },
      "outputs": [],
      "source": [
        "negative_review = clean_df[clean_df['polarity'] == 'negative'] # Memfilter hanya data dengan polaritas negatif\n",
        "negative_review = negative_review[['text_akhir', 'polarity_score', 'polarity','text_stopword']] # Memilih kolom yang ingin ditampilkan\n",
        "negative_review = negative_review.sort_values(by='polarity_score', ascending=False) # Mengurutkan berdasarkan skor polaritas secara menurun\n",
        "negative_review = negative_review.reset_index(drop=True) # Mengatur ulang indeks\n",
        "negative_review.index += 1 # Menambahkan nomor urut dari 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0c6cqgF0Ic3"
      },
      "outputs": [],
      "source": [
        "neutral_review = clean_df[clean_df['polarity'] == 'neutral'] # Memfilter hanya data dengan polaritas netral\n",
        "neutral_review = neutral_review[['text_akhir', 'polarity_score', 'polarity','text_stopword']] # Memilih kolom yang ingin ditampilkan\n",
        "neutral_review = neutral_review.sort_values(by='polarity_score', ascending=False) # Mengurutkan berdasarkan skor polaritas secara menurun\n",
        "neutral_review = neutral_review.reset_index(drop=True) # Mengatur ulang indeks\n",
        "neutral_review.index += 1 # Menambahkan nomor urut dari 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4XWPsTM0Nll"
      },
      "outputs": [],
      "source": [
        "list_words = '' # Menggabungkan semua kata dari kolom 'text_stopword' menjadi satu string\n",
        "for tweet in clean_df['text_stopword']:  # Mengiterasi melalui setiap tweet dalam kolom 'text_stopword' dari clean_df\n",
        "    for word in tweet:  # Mengiterasi melalui setiap kata dalam setiap tweet\n",
        "        list_words += ' ' + (word)  # Menambahkan kata ke dalam list_words dengan spasi di antaranya\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words) # Membuat WordCloud berdasarkan list_words\n",
        "fig, ax = plt.subplots(figsize=(10, 8)) # Membuat figure dan axis untuk plot\n",
        "ax.set_title('Word Cloud Review Semua Polarity', fontsize=20) # Menetapkan judul pada plot\n",
        "ax.grid(False) # Menghilangkan grid pada plot\n",
        "ax.imshow((wordcloud)) # Menampilkan WordCloud\n",
        "fig.tight_layout(pad=0) # Menyesuaikan layout figure\n",
        "ax.axis('off') # Menyembunyikan axis\n",
        "plt.show() # Menampilkan plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqF1drYl0W5V"
      },
      "outputs": [],
      "source": [
        "list_words = '' # Menggabungkan semua kata dari kolom 'text_stopword' menjadi satu string\n",
        "for review in positive_review['text_stopword']:  # Mengiterasi melalui setiap review dalam kolom 'text_stopword' dari positive_review\n",
        "    for word in review:  # Mengiterasi melalui setiap kata dalam setiap review\n",
        "        list_words += ' ' + (word)  # Menambahkan kata ke dalam list_words dengan spasi di antaranya\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words) # Membuat WordCloud berdasarkan list_words\n",
        "fig, ax = plt.subplots(figsize=(10, 8)) # Membuat figure dan axis untuk plot\n",
        "ax.set_title('Word Cloud Positive Review', fontsize=20) # Menetapkan judul pada plot\n",
        "ax.grid(False) # Menghilangkan grid pada plot\n",
        "ax.imshow((wordcloud)) # Menampilkan WordCloud\n",
        "fig.tight_layout(pad=0) # Menyesuaikan layout figure\n",
        "ax.axis('off') # Menyembunyikan axis\n",
        "plt.show() # Menampilkan plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9ESPn2p0mVg"
      },
      "outputs": [],
      "source": [
        "list_words = '' # Menggabungkan semua kata dari kolom 'text_stopword' menjadi satu string\n",
        "for review in negative_review['text_stopword']:  # Mengiterasi melalui setiap review dalam kolom 'text_stopword' dari negative_review\n",
        "    for word in review:  # Mengiterasi melalui setiap kata dalam setiap review\n",
        "        list_words += ' ' + (word)  # Menambahkan kata ke dalam list_words dengan spasi di antaranya\n",
        "\n",
        "wordcloud = WordCloud(width=600, height=400, background_color='white', min_font_size=10).generate(list_words) # Membuat WordCloud berdasarkan list_words\n",
        "fig, ax = plt.subplots(figsize=(10, 8)) # Membuat figure dan axis untuk plot\n",
        "ax.set_title('Word Cloud Negative Review', fontsize=20) # Menetapkan judul pada plot\n",
        "ax.grid(False) # Menghilangkan grid pada plot\n",
        "ax.imshow((wordcloud)) # Menampilkan WordCloud\n",
        "fig.tight_layout(pad=0) # Menyesuaikan layout figure\n",
        "ax.axis('off') # Menyembunyikan axis\n",
        "plt.show() # Menampilkan plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEOj0qJh0sr4"
      },
      "source": [
        "# **4.3 Distribusi Kelas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkAqcCmB0yDE"
      },
      "outputs": [],
      "source": [
        "# Mengatur ukuran gambar plot\n",
        "plt.figure(figsize=(8, 9))\n",
        "\n",
        "# Membuat plot count menggunakan seaborn untuk variabel 'polarity' dari dataframe 'clean_df'\n",
        "class_dist_plot = sns.countplot(x='polarity', data=clean_df)\n",
        "\n",
        "# Menetapkan judul plot\n",
        "plt.title('Distribusi Kelas')\n",
        "\n",
        "# Menetapkan label sumbu x\n",
        "plt.xlabel('Polarity')\n",
        "\n",
        "# Menetapkan label sumbu y\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Menambahkan label angka untuk setiap bar pada plot\n",
        "for p in class_dist_plot.patches:\n",
        "    class_dist_plot.annotate(format(p.get_height(), '.0f'),\n",
        "                             (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                             ha='center', va='center',\n",
        "                             xytext=(0, 10),\n",
        "                             textcoords='offset points')\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvtskper047Q"
      },
      "source": [
        "# **5. Pemodelan**\n",
        "\n",
        "## **5.1 Data Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKM1EQSq1IoT"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk2Y1zG02CjN"
      },
      "source": [
        "## **5.2 Support Vector Machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLwwMV4H2M2l"
      },
      "outputs": [],
      "source": [
        "# Membuat objek model SVM\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Melatih model SVM pada data pelatihan\n",
        "start_time = time.time()\n",
        "svm.fit(X_train.toarray(), y_train)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "# Prediksi sentimen pada data pelatihan dan data uji\n",
        "y_pred_train_svm = svm.predict(X_train.toarray())\n",
        "y_pred_test_svm = svm.predict(X_test.toarray())\n",
        "\n",
        "# Evaluasi akurasi model SVM pada data pelatihan\n",
        "accuracy_train_svm = accuracy_score(y_pred_train_svm, y_train)\n",
        "\n",
        "# Evaluasi akurasi model SVM pada data uji\n",
        "accuracy_test_svm = accuracy_score(y_pred_test_svm, y_test)\n",
        "\n",
        "# Menampilkan akurasi dan waktu eksekusi\n",
        "print('SVM - accuracy_train:', accuracy_train_svm)\n",
        "print('SVM - accuracy_test:', accuracy_test_svm)\n",
        "print('SVM - execution_time:', execution_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvBQx6D0-CNL"
      },
      "source": [
        "## **5.3 LightGBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkQxAHH--H0g"
      },
      "outputs": [],
      "source": [
        "# Membuat objek model LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Melatih model LightGBM pada data pelatihan\n",
        "start_time = time.time()\n",
        "lgb_model.fit(X_train.toarray(), y_train)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "# Prediksi sentimen pada data pelatihan dan data uji\n",
        "y_pred_train_lgb = lgb_model.predict(X_train.toarray())\n",
        "y_pred_test_lgb = lgb_model.predict(X_test.toarray())\n",
        "\n",
        "# Evaluasi akurasi model LightGBM pada data pelatihan\n",
        "accuracy_train_lgb = accuracy_score(y_pred_train_lgb, y_train)\n",
        "\n",
        "# Evaluasi akurasi model LightGBM pada data uji\n",
        "accuracy_test_lgb = accuracy_score(y_pred_test_lgb, y_test)\n",
        "\n",
        "# Menampilkan akurasi dan waktu eksekusi\n",
        "print('LightGBM - accuracy_train:', accuracy_train_lgb)\n",
        "print('LightGBM - accuracy_test:', accuracy_test_lgb)\n",
        "print('LightGBM - execution_time:', execution_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0svoysIi-PqK"
      },
      "source": [
        "## **5.4 XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4nJiQjn-Xoe"
      },
      "outputs": [],
      "source": [
        "# Mengonversi sparse matrix ke array\n",
        "X_train_array = X_train.toarray()\n",
        "X_test_array = X_test.toarray()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Membuat objek model XGBoost\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Melatih model XGBoost pada data pelatihan\n",
        "start_time = time.time()\n",
        "xgb.fit(X_train_array, y_train_encoded)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "# Prediksi sentimen pada data pelatihan dan data uji\n",
        "y_pred_train_xgb = xgb.predict(X_train_array)\n",
        "y_pred_test_xgb = xgb.predict(X_test_array)\n",
        "\n",
        "# Evaluasi akurasi model XGBoost pada data pelatihan\n",
        "accuracy_train_xgb = accuracy_score(y_pred_train_xgb, y_train_encoded)\n",
        "\n",
        "# Evaluasi akurasi model XGBoost pada data uji\n",
        "accuracy_test_xgb = accuracy_score(y_pred_test_xgb, y_test_encoded)\n",
        "\n",
        "# Menampilkan akurasi dan waktu eksekusi\n",
        "print('XGBoost - accuracy_train:', accuracy_train_xgb)\n",
        "print('XGBoost - accuracy_test:', accuracy_test_xgb)\n",
        "print('XGBoost - execution_time:', execution_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELGNcp4A-cvF"
      },
      "source": [
        "## **5.5 CatBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM3Emky3-j2S"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import time\n",
        "\n",
        "# Membuat objek model CatBoost\n",
        "cat_model = CatBoostClassifier(n_estimators=100, random_state=42, verbose=0)\n",
        "\n",
        "# Melatih model CatBoost pada data pelatihan\n",
        "start_time = time.time()\n",
        "cat_model.fit(X_train.toarray(), y_train)\n",
        "execution_time = time.time() - start_time\n",
        "\n",
        "# Prediksi sentimen pada data pelatihan dan data uji\n",
        "y_pred_train_cat = cat_model.predict(X_train.toarray())\n",
        "y_pred_test_cat = cat_model.predict(X_test.toarray())\n",
        "\n",
        "# Evaluasi akurasi model CatBoost pada data pelatihan\n",
        "accuracy_train_cat = accuracy_score(y_pred_train_cat, y_train)\n",
        "\n",
        "# Evaluasi akurasi model CatBoost pada data uji\n",
        "accuracy_test_cat = accuracy_score(y_pred_test_cat, y_test)\n",
        "\n",
        "# Menampilkan akurasi dan waktu eksekusi\n",
        "print('CatBoost - accuracy_train:', accuracy_train_cat)\n",
        "print('CatBoost - accuracy_test:', accuracy_test_cat)\n",
        "print('CatBoost - execution_time:', execution_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX34NL8o-pEm"
      },
      "source": [
        "## **5.6 Perbandingan Akurasi Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvnO_mFO-vam"
      },
      "outputs": [],
      "source": [
        "# Membuat DataFrame untuk hasil akurasi\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': ['Support Vector Machine','Light Gradient-Boosting Machine','Extreme Gradient Boosting','Categorical Boosting'],\n",
        "    'Accuracy Train': [accuracy_train_svm, accuracy_train_lgb, accuracy_train_xgb, accuracy_train_cat],\n",
        "    'Accuracy Test': [accuracy_test_svm, accuracy_test_lgb, accuracy_test_xgb, accuracy_test_cat]\n",
        "})\n",
        "\n",
        "# Menampilkan hanya kolom \"Accuracy Test\"\n",
        "accuracy_test_only = results_df[['Model', 'Accuracy Test']]\n",
        "\n",
        "# Mengurutkan DataFrame berdasarkan kolom \"Accuracy Test\" dari tertinggi ke terendah\n",
        "accuracy_test_sorted = accuracy_test_only.sort_values(by='Accuracy Test', ascending=False)\n",
        "\n",
        "# Menampilkan DataFrame yang telah diurutkan\n",
        "print(accuracy_test_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PzH3rxRj5xK"
      },
      "source": [
        "# **6. KESIMPULAN**\n",
        "\n",
        "Berdasarkan perbandingan tersebut terlihat Support Vector Machine memiliki akurasi paling tinggi dari algoritma lainnya yaitu 87.13%.\n",
        "\n",
        "Selanjutnya, pemodelan akan digunakan untuk pengetesan inference dengan pertama-tama Menyimpan Model yang Terlatih dengan akurasi paling tinggi dan Objek TF-IDF di **Pemodelan.ipynb**, dan berikutnya Memuat Model yang Terlatih dan Objek TF-IDF di **Testing.ipynb**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kp5iYEEj5xM"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan model SVM\n",
        "with open('svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svm, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5XMoH9Gj5xN"
      },
      "outputs": [],
      "source": [
        "# Simpan TfidfVectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **-----------------------------------------------------------------------------**"
      ],
      "metadata": {
        "id": "_e5T40TckA9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING MODEL**\n",
        "\n",
        "### **1. PREPARATION**"
      ],
      "metadata": {
        "id": "1WaVrEx3TWqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "import pickle\n",
        "from pickle import UnpicklingError\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Muat model SVM\n",
        "try:\n",
        "    with open('/content/svm_model.pkl', 'rb') as f:\n",
        "        svm = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading SVM model: {e}\")\n",
        "    # Additional diagnostic steps:\n",
        "    print(\"Periksa kembali apakah 'svm_model.pkl' ada dan tidak kosong.\")\n",
        "    print(\"Jika file model berasal dari transfer data, pastikan sudah berada pada binary mode.\")\n",
        "\n",
        "# Muat TfidfVectorizer\n",
        "try:\n",
        "    with open('/content/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        tfidf = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading TF-IDF vectorizer: {e}\")\n",
        "\n",
        "# Fungsi pra-pemrosesan teks (sama seperti di Pemodelan.ipynb)\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Menghapus mention (@username)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)  # Menghapus hashtag (#hashtag)\n",
        "    text = re.sub(r\"http\\S+\", '', text)  # Menghapus URL\n",
        "    text = re.sub(r'[0-9]+', '', text)  # Menghapus angka\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Menghapus karakter non-alfanumerik kecuali spasi\n",
        "    text = text.replace('\\n', ' ')  # Mengganti newline dengan spasi\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Menghapus tanda baca\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"livin\", \"mandiri\", \"mandiri mobile\"]])  # Menghapus kata-kata yang diinginkan\n",
        "    text = text.strip(' ')  # Menghapus spasi di awal dan akhir teks\n",
        "    return text  # Mengembalikan teks yang telah dibersihkan\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()  # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
        "    return text  # Mengembalikan teks yang telah di-casefold\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)  # Memecah teks menjadi token (kata-kata)\n",
        "    return text  # Mengembalikan daftar token\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))  # Mengambil daftar stopwords bahasa Indonesia\n",
        "    listStopwords1 = set(stopwords.words('english'))  # Mengambil daftar stopwords bahasa Inggris\n",
        "    listStopwords.update(listStopwords1)  # Menggabungkan dua daftar stopwords\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:  # Memeriksa apakah kata bukan stopword\n",
        "            filtered.append(txt)  # Menambahkan kata yang bukan stopword ke daftar filtered\n",
        "    text = filtered  # Menggantikan teks dengan daftar kata yang telah difilter\n",
        "    return text  # Mengembalikan daftar kata yang telah difilter\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()  # Membuat pabrik stemmer\n",
        "    stemmer = factory.create_stemmer()  # Membuat stemmer\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]  # Melakukan stemming pada setiap kata\n",
        "    stemmed_text = ' '.join(stemmed_words)  # Menggabungkan kata-kata yang telah distem menjadi satu kalimat\n",
        "    return stemmed_text  # Mengembalikan teks yang telah distem\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)  # Menggabungkan daftar kata menjadi satu kalimat\n",
        "    return sentence  # Mengembalikan kalimat yang dibentuk dari daftar kata\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()  # Memecah teks menjadi kata-kata\n",
        "    fixed_words = []  # Inisialisasi daftar untuk kata-kata yang sudah diperbaiki\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:  # Memeriksa apakah kata tersebut adalah kata slang\n",
        "            fixed_words.append(slangwords[word.lower()])  # Jika ya, mengganti kata slang dengan kata standar\n",
        "        else:\n",
        "            fixed_words.append(word)  # Jika bukan kata slang atau kata yang ingin dihapus, tetap mempertahankan kata tersebut\n",
        "    fixed_text = ' '.join(fixed_words)  # Menggabungkan kata-kata yang sudah diperbaiki menjadi satu teks\n",
        "    return fixed_text  # Mengembalikan teks yang sudah diperbaiki\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sang-ichal/datasetlivin/main/slangwords.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)  # Mengirim permintaan HTTP GET ke URL untuk mendapatkan konten\n",
        "\n",
        "if response.status_code == 200:  # Jika status code respons adalah 200 (OK)\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)  # Mengurai JSON yang diperoleh dari respons dan menyimpannya dalam variabel slangwords\n",
        "    except json.JSONDecodeError as e: # Menangani kesalahan jika terjadi masalah saat mengurai JSON\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code) # Menampilkan pesan kesalahan jika tidak berhasil mengambil data dari URL\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = cleaningText(text)\n",
        "    text = casefoldingText(text)\n",
        "    text = fix_slangwords(text)\n",
        "    text = tokenizingText(text)\n",
        "    text = filteringText(text)\n",
        "    text = toSentence(text)\n",
        "    return text\n",
        "\n",
        "def prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm):\n",
        "    # Melakukan preprocessing pada kalimat baru\n",
        "    review_baru_cleaned = cleaningText(review_baru)\n",
        "    review_baru_casefolded = casefoldingText(review_baru_cleaned)\n",
        "    review_baru_slangfixed = fix_slangwords(review_baru_casefolded)\n",
        "    review_baru_tokenized = tokenizingText(review_baru_slangfixed)\n",
        "    review_baru_filtered = filteringText(review_baru_tokenized)\n",
        "    review_baru_final = toSentence(review_baru_filtered)\n",
        "\n",
        "    # Menggunakan objek tfidf yang sudah di-fit dari pelatihan sebelumnya\n",
        "    X_review_baru = tfidf.transform([review_baru_final])\n",
        "\n",
        "    # Convert sparse matrix to dense array\n",
        "    X_review_baru = X_review_baru.toarray()\n",
        "\n",
        "    # Memperoleh prediksi sentimen review baru menggunakan model terbaik\n",
        "    prediksi_sentimen = svm.predict(X_review_baru)\n",
        "\n",
        "    # Menampilkan hasil prediksi\n",
        "    if prediksi_sentimen[0] == 'positive':\n",
        "        hasil = \"Sentimen review baru adalah POSITIF.\"\n",
        "    elif prediksi_sentimen[0] == 'negative':\n",
        "        hasil = \"Sentimen review baru adalah NEGATIF.\"\n",
        "    else:\n",
        "        hasil = \"Sentimen review baru adalah NETRAL.\"\n",
        "\n",
        "    return hasil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB0X7N4qTdE0",
        "outputId": "4b8c9c17-d9bd-433f-e15c-594cf9d42bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2 Kategori Negatif**"
      ],
      "metadata": {
        "id": "pRgBi40_bzNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"hang mulu loginnya nih apps gagal mulu\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RWPByN13b15E",
        "outputId": "09c1bbd2-875d-4d30-aebf-6b9ad4bc46b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NEGATIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3 Kategori Positif**"
      ],
      "metadata": {
        "id": "14wLgABSb6UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"Aplikasinya mantap, membantu transaksi saya tdk pernah gagal\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k9YF3F10b8zM",
        "outputId": "b31cda72-6869-4b0c-f265-017bd281be1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Kategori Netral**"
      ],
      "metadata": {
        "id": "6TEo7Q66b_Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"Sudah oke\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-X5gW9YpcTS_",
        "outputId": "18b585c3-96fd-4251-c2fc-33dd26410651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NETRAL.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REQUIREMENT.TXT**"
      ],
      "metadata": {
        "id": "pqSS-g9mh4K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > /content/requirements.txt"
      ],
      "metadata": {
        "id": "s2wEo7nfh79h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZTyV7fO-1km"
      },
      "source": [
        "# **6. KESIMPULAN**\n",
        "\n",
        "Berdasarkan perbandingan tersebut terlihat [algoritma ] memiliki akurasi paling tinggi dari algoritma lainnya yaitu ...... %.\n",
        "\n",
        "Selanjutnya, pemodelan akan digunakan untuk pengetesan inference dengan pertama-tama Menyimpan Model yang Terlatih dengan akurasi paling tinggi dan Objek TF-IDF di **Pemodelan.ipynb**, dan berikutnya Memuat Model yang Terlatih dan Objek TF-IDF di **Testing.ipynb**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qjs3FVQ-6KL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan model SVM\n",
        "with open('svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svm, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD3Xlfif_Whq"
      },
      "outputs": [],
      "source": [
        "# Simpan TfidfVectorizer\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyqFVfNv8vJu"
      },
      "outputs": [],
      "source": [
        "X = clean_df['text_akhir'] # Menyimpan nilai pada dataframe text_akhir ke variabel X\n",
        "y = clean_df['polarity'] # Menyimpan nilai pada dataframe polarity ke variabel y"
      ]
    }
  ]
}